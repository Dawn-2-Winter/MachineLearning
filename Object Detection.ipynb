{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nUCDNdnBkIXI",
        "3-cX8ofTkLZX",
        "fNsbwdfykPo_",
        "xx0A-jmMkWZi",
        "Fwm68kNbj7-L",
        "B5KdVkjf2TlB",
        "Nedl1D1Xg_wa",
        "cHM_XVEScwj_"
      ],
      "mount_file_id": "1cdk7JH4QqYQdj-GbeWktBw0tuCyXtix4",
      "authorship_tag": "ABX9TyMvXMLrD2maAWe/GUAJr0Yp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dawn-2-Winter/MachineLearning/blob/master/Object%20Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/deeplearning_assignment_1_dataset"
      ],
      "metadata": {
        "id": "Y9TAls7geZTB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! unzip /content/drive/MyDrive/Colab_Notebooks/deeplearning_assignment_1_dataset.zip"
      ],
      "metadata": {
        "id": "EXViR-LkfOc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3FLLFlh1haI",
        "outputId": "9ae2578e-42a4-4333-db78-eeb8ff722660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HKU-DASC7606-A1'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 37 (delta 10), reused 23 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (37/37), 1.05 MiB | 6.39 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Liang-ZX/HKU-DASC7606-A1.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PreDefine"
      ],
      "metadata": {
        "id": "392F0m_chzZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import argparse\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import json\n",
        "import random\n",
        "import cv2\n",
        "import torchvision\n",
        "import pickle\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "H3FGv96P1kb_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def redefine_conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "def redefine_conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                     padding=0, bias=False)\n"
      ],
      "metadata": {
        "id": "u1CU0k1ZGpMW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet Backbone"
      ],
      "metadata": {
        "id": "nUCDNdnBkIXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = redefine_conv3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = redefine_conv3x3(planes, planes * BasicBlock.expansion)\n",
        "        self.bn2 = nn.BatchNorm2d(planes * BasicBlock.expansion)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "r8xD-c1vHAXt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        ##############################################################\n",
        "        # TODO: Please define your layers with the BottleNeck from the paper \"Deep Residual Learning for Image Recognition\"\n",
        "        #\n",
        "        # Note: You **must not** use the nn.Conv2d here but use **redefine_conv3x3** and **redefine_conv1x1** in this script instead\n",
        "        ##############################################################\n",
        "        self.conv1 = redefine_conv1x1(in_planes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = redefine_conv3x3(planes, planes)\n",
        "        self.conv3 = redefine_conv1x1(planes, planes * Bottleneck.expansion)\n",
        "        self.bn2 = nn.BatchNorm2d(planes * Bottleneck.expansion)\n",
        "        ###############################################################\n",
        "        self.downsample = downsample\n",
        "\n",
        "        if stride != 1 or in_planes != planes * Bottleneck.expansion:\n",
        "          self.downsample = nn.Sequential(\n",
        "              nn.Conv2d(in_planes, planes * Bottleneck.expansion, stride=stride, kernel_size=1, bias=False),\n",
        "              nn.BatchNorm2d(planes * Bottleneck.expansion)\n",
        "          )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        ##############################################################\n",
        "        # TODO: Please write the forward function with your defined layers\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn2(out)\n",
        "        # print('out shape', out.shape)\n",
        "        ##############################################################\n",
        "        # out = x    you can delete this line if it's not needed\n",
        "\n",
        "        ###############################################################\n",
        "        if self.downsample is not None:\n",
        "            # print('downsample is not None')\n",
        "            residual = self.downsample(x)\n",
        "        # print('residual shape', residual.shape)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "foX2zN2kHCvQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mmdetection_head"
      ],
      "metadata": {
        "id": "3-cX8ofTkLZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class mmdetection_head(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, do_downsample=False):\n",
        "        super(mmdetection_head, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=2, bias=True, dilation=2)  # dilation\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=True)\n",
        "\n",
        "        ####################################################################\n",
        "        # TODO: Please complete the downsample module\n",
        "        # Hint: Use a \"kernel_size=1\"'s convolution layer to align the dimension\n",
        "        # Hint: We don't suggest using any batch normalization on detection head.\n",
        "        #####################################################################\n",
        "        self.downsample = nn.Sequential()\n",
        "        if do_downsample or stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride = stride)\n",
        "            )\n",
        "\n",
        "        ##################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out += self.downsample(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "A7U-iwOW-Xmh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model"
      ],
      "metadata": {
        "id": "oUHhTktQAjeI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet"
      ],
      "metadata": {
        "id": "fNsbwdfykPo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, args): \n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        ###################################################################\n",
        "        # TODO: Please fill the codes below with the *self._make_layer()* function\n",
        "        ##################################################################\n",
        "        self.conv2 = self._make_layer(block, 64, layers[0], 1)\n",
        "        self.conv3 = self._make_layer(block, 128, layers[1], 1)\n",
        "        self.conv4 = self._make_layer(block, 256, layers[2], 1)\n",
        "        self.conv5 = self._make_layer(block, 512, layers[3], 1)\n",
        "\n",
        "        # adjust = redefine_conv1x1()\n",
        "        ##################################################################\n",
        "\n",
        "\n",
        "        ###################################################################\n",
        "        # TODO: Please substitute the \"?\" with specific numbers\n",
        "        ###################################################################\n",
        "        # change ResNet to suit the detection requirement\n",
        "        yolo_S, yolo_B, yolo_C = args['yolo_S'], args['yolo_B'], args['yolo_C']\n",
        "\n",
        "        self.det_head = self._make_detection_head(in_channels=512*block.expansion, out_channels=yolo_B*5+yolo_C)\n",
        "        ###################################################################\n",
        "                \n",
        "        def _weights_init(m):\n",
        "            \"\"\" kaiming init (https://arxiv.org/abs/1502.01852v1)\"\"\"\n",
        "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        \n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_planes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "        layers = [block(self.in_planes, planes, stride, downsample)]\n",
        "        self.in_planes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.in_planes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_detection_head(self, in_channels, out_channels):\n",
        "        layers = [\n",
        "            mmdetection_head(in_planes=in_channels, planes=256, do_downsample=True),\n",
        "            mmdetection_head(in_planes=256, planes=256),\n",
        "            mmdetection_head(in_planes=256, planes=256),\n",
        "            nn.Conv2d(256, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        ]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        ###################################################################\n",
        "        # TODO: Please fill the codes below\n",
        "        ##################################################################\n",
        "        # print('after conv1', x.shape)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv5(x)\n",
        "        \n",
        "        ##################################################################\n",
        "\n",
        "        x = self.det_head(x)\n",
        "\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "\n",
        "        return x\n",
        "        ##################################################################"
      ],
      "metadata": {
        "id": "QszuGcixAHGW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model define"
      ],
      "metadata": {
        "id": "xx0A-jmMkWZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
        "    return model"
      ],
      "metadata": {
        "id": "8YvIqLMjL_nX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model"
      ],
      "metadata": {
        "id": "djv9cbFDkxDO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet101(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    return model"
      ],
      "metadata": {
        "id": "zZ4gK18sLgRG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "    return model"
      ],
      "metadata": {
        "id": "qXrGoTjFL8tY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = dict()\n",
        "args['yolo_S'] = 14\n",
        "args['yolo_B'] = 2\n",
        "args['yolo_C'] = 5\n",
        "args['num_epochs'] = 1\n",
        "args['batch_size'] = 20\n",
        "args['learning_rate'] = 1e-5\n",
        "args['seed'] = 666\n",
        "args['dataset_root'] = '/content/deeplearning_assignment_1_dataset'\n",
        "args['output_dir'] = '/content/checkpoints'\n",
        "args['l_coord'] = 5.\n",
        "args['l_noobj'] = 0.5\n",
        "args['nms_threshold'] = 0.5\n",
        "args['image_path'] = '/content/deeplearning_assignment_1_dataset/val/image/000001.jpg'\n",
        "args['model_path'] = \"/content/checkpoints/hku_mmdetector_best.pth\"\n",
        "args['unsave_img'] = False\n",
        "args['vis_dir'] = '/content/vis_results'\n",
        "args['nms_threshold'] =0.5\n",
        "args['split'] = 'val'     # val/test\n",
        "args['output_file'] = \"/content/checkpoints/result.pkl\"\n",
        "args['pos_threshold'] = 0.3\n",
        "\n",
        "CAR_CLASSES = ['Pedestrian', 'Cyclist', 'Car', 'Truck','Tram']\n",
        "\n",
        "COLORS = {'Pedestrian': (0, 0, 0),\n",
        "          'Cyclist': (128, 0, 0),\n",
        "          'Car': (0, 128, 0),\n",
        "          'Truck': (128, 128, 0),\n",
        "          'Tram': (0, 0, 128)}"
      ],
      "metadata": {
        "id": "XCnkyqILhuSe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hku_mmdetector34 = resnet34(args = args)\n",
        "hku_mmdetector50 = resnet50(args = args)\n",
        "hku_mmdetector101 = resnet101(args = args)"
      ],
      "metadata": {
        "id": "TvcDai2KMCse"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataSet"
      ],
      "metadata": {
        "id": "Fwm68kNbj7-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BGR2RGB(img):\n",
        "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "def BGR2HSV(img):\n",
        "    return cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "\n",
        "def HSV2BGR(img):\n",
        "    return cv2.cvtColor(img, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "\n",
        "def RandomBrightness(bgr):\n",
        "    if random.random() < 0.5:\n",
        "        hsv = BGR2HSV(bgr)\n",
        "        h, s, v = cv2.split(hsv)\n",
        "        adjust = random.choice([0.5, 1.5])\n",
        "        v = v * adjust\n",
        "        v = np.clip(v, 0, 255).astype(hsv.dtype)\n",
        "        hsv = cv2.merge((h, s, v))\n",
        "        bgr = HSV2BGR(hsv)\n",
        "    return bgr\n",
        "\n",
        "\n",
        "def RandomSaturation(bgr):\n",
        "    if random.random() < 0.5:\n",
        "        hsv = BGR2HSV(bgr)\n",
        "        h, s, v = cv2.split(hsv)\n",
        "        adjust = random.choice([0.5, 1.5])\n",
        "        s = s * adjust\n",
        "        s = np.clip(s, 0, 255).astype(hsv.dtype)\n",
        "        hsv = cv2.merge((h, s, v))\n",
        "        bgr = HSV2BGR(hsv)\n",
        "    return bgr\n",
        "\n",
        "\n",
        "def RandomHue(bgr):\n",
        "    if random.random() < 0.5:\n",
        "        hsv = BGR2HSV(bgr)\n",
        "        h, s, v = cv2.split(hsv)\n",
        "        adjust = random.choice([0.5, 1.5])\n",
        "        h = h * adjust\n",
        "        h = np.clip(h, 0, 255).astype(hsv.dtype)\n",
        "        hsv = cv2.merge((h, s, v))\n",
        "        bgr = HSV2BGR(hsv)\n",
        "    return bgr\n",
        "\n",
        "\n",
        "def randomBlur(bgr):\n",
        "    if random.random() < 0.5:\n",
        "        bgr = cv2.blur(bgr, (5, 5))\n",
        "    return bgr\n",
        "\n",
        "\n",
        "def randomShift(bgr, boxes, labels):\n",
        "    center = (boxes[:, 2:] + boxes[:, :2]) / 2\n",
        "    if random.random() < 0.5:\n",
        "        height, width, c = bgr.shape\n",
        "        after_shfit_image = np.zeros((height, width, c), dtype=bgr.dtype)\n",
        "        after_shfit_image[:, :, :] = (104, 117, 123)  # bgr\n",
        "        shift_x = random.uniform(-width * 0.2, width * 0.2)\n",
        "        shift_y = random.uniform(-height * 0.2, height * 0.2)\n",
        "\n",
        "        if shift_x >= 0 and shift_y >= 0:\n",
        "            after_shfit_image[int(shift_y):, int(shift_x):, :] = bgr[:height - int(shift_y), :width - int(shift_x),\n",
        "                                                                 :]\n",
        "        elif shift_x >= 0 and shift_y < 0:\n",
        "            after_shfit_image[:height + int(shift_y), int(shift_x):, :] = bgr[-int(shift_y):, :width - int(shift_x),\n",
        "                                                                          :]\n",
        "        elif shift_x < 0 and shift_y >= 0:\n",
        "            after_shfit_image[int(shift_y):, :width + int(shift_x), :] = bgr[:height - int(shift_y), -int(shift_x):,\n",
        "                                                                         :]\n",
        "        elif shift_x < 0 and shift_y < 0:\n",
        "            after_shfit_image[:height + int(shift_y), :width + int(shift_x), :] = bgr[-int(shift_y):,\n",
        "                                                                                  -int(shift_x):, :]\n",
        "\n",
        "        shift_xy = torch.FloatTensor([[int(shift_x), int(shift_y)]]).expand_as(center)\n",
        "        center = center + shift_xy\n",
        "        mask1 = (center[:, 0] > 0) & (center[:, 0] < width)\n",
        "        mask2 = (center[:, 1] > 0) & (center[:, 1] < height)\n",
        "        mask = (mask1 & mask2).view(-1, 1)\n",
        "        boxes_in = boxes[mask.expand_as(boxes)].view(-1, 4)\n",
        "        if len(boxes_in) == 0:\n",
        "            return bgr, boxes, labels\n",
        "        box_shift = torch.FloatTensor([[int(shift_x), int(shift_y), int(shift_x), int(shift_y)]]).expand_as(\n",
        "            boxes_in)\n",
        "        boxes_in = boxes_in + box_shift\n",
        "        labels_in = labels[mask.view(-1)]\n",
        "        return after_shfit_image, boxes_in, labels_in\n",
        "    return bgr, boxes, labels\n",
        "\n",
        "\n",
        "def randomScale(bgr, boxes):\n",
        "    if random.random() < 0.5:\n",
        "        scale = random.uniform(0.8, 1.2)\n",
        "        height, width, c = bgr.shape\n",
        "        bgr = cv2.resize(bgr, (int(width * scale), height))\n",
        "        scale_tensor = torch.FloatTensor([[scale, 1, scale, 1]]).expand_as(boxes)\n",
        "        boxes = boxes * scale_tensor\n",
        "        return bgr, boxes\n",
        "    return bgr, boxes\n",
        "\n",
        "\n",
        "def randomCrop(bgr, boxes, labels):\n",
        "    if random.random() < 0.5:\n",
        "        center = (boxes[:, 2:] + boxes[:, :2]) / 2\n",
        "        height, width, c = bgr.shape\n",
        "        h = random.uniform(0.6 * height, height)\n",
        "        w = random.uniform(0.6 * width, width)\n",
        "        x = random.uniform(0, width - w)\n",
        "        y = random.uniform(0, height - h)\n",
        "        x, y, h, w = int(x), int(y), int(h), int(w)\n",
        "\n",
        "        center = center - torch.FloatTensor([[x, y]]).expand_as(center)\n",
        "        mask1 = (center[:, 0] > 0) & (center[:, 0] < w)\n",
        "        mask2 = (center[:, 1] > 0) & (center[:, 1] < h)\n",
        "        mask = (mask1 & mask2).view(-1, 1)\n",
        "\n",
        "        boxes_in = boxes[mask.expand_as(boxes)].view(-1, 4)\n",
        "        if len(boxes_in) == 0:\n",
        "            return bgr, boxes, labels\n",
        "        box_shift = torch.FloatTensor([[x, y, x, y]]).expand_as(boxes_in)\n",
        "\n",
        "        boxes_in = boxes_in - box_shift\n",
        "        boxes_in[:, 0] = boxes_in[:, 0].clamp_(min=0, max=w)\n",
        "        boxes_in[:, 2] = boxes_in[:, 2].clamp_(min=0, max=w)\n",
        "        boxes_in[:, 1] = boxes_in[:, 1].clamp_(min=0, max=h)\n",
        "        boxes_in[:, 3] = boxes_in[:, 3].clamp_(min=0, max=h)\n",
        "\n",
        "        labels_in = labels[mask.view(-1)]\n",
        "        img_croped = bgr[y:y + h, x:x + w, :]\n",
        "        return img_croped, boxes_in, labels_in\n",
        "    return bgr, boxes, labels\n",
        "\n",
        "\n",
        "def subMean(bgr, mean):\n",
        "    mean = np.array(mean, dtype=np.float32)\n",
        "    bgr = bgr - mean\n",
        "    return bgr\n",
        "\n",
        "\n",
        "def subMeanDividedStd(rgb, mean, std):\n",
        "    mean = np.array(mean, dtype=np.float32)\n",
        "    std = np.array(std, dtype=np.float32)\n",
        "    rgb = (rgb - mean) / std\n",
        "    return rgb\n",
        "\n",
        "\n",
        "def random_flip(im, boxes):\n",
        "    if random.random() < 0.5:\n",
        "        im_lr = np.fliplr(im).copy()\n",
        "        h, w, _ = im.shape\n",
        "        xmin = w - boxes[:, 2]\n",
        "        xmax = w - boxes[:, 0]\n",
        "        boxes[:, 0] = xmin\n",
        "        boxes[:, 2] = xmax\n",
        "        return im_lr, boxes\n",
        "    return im, boxes\n",
        "\n",
        "\n",
        "def random_bright(im, delta=16):\n",
        "    alpha = random.random()\n",
        "    if alpha > 0.3:\n",
        "        im = im * alpha + random.randrange(-delta, delta)\n",
        "        im = im.clip(min=0, max=255).astype(np.uint8)\n",
        "    return im\n",
        "\n",
        "\n",
        "def load_json(path):\n",
        "    with open(path, mode=\"r\") as f:\n",
        "        data = json.load(f)\n",
        "    return data"
      ],
      "metadata": {
        "id": "4bo02LUQgW69"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(data.Dataset):\n",
        "    image_size = 448\n",
        "\n",
        "    def __init__(self, args, split, transform):\n",
        "        print('DATASET INITIALIZATION')\n",
        "        self.args = args\n",
        "        root = args\n",
        "        self.root_images = os.path.join(root, split, 'image')\n",
        "        if split == \"train\":\n",
        "            self.train = True\n",
        "        else:\n",
        "            self.train = False\n",
        "\n",
        "        self.transform = transform\n",
        "        self.f_names, self.boxes, self.labels = [], [], []\n",
        "        self.mean = [123.675, 116.280, 103.530]  # RGB\n",
        "        self.std = [58.395, 57.120, 57.375]\n",
        "        annotation_path = os.path.join(root, 'annotations', 'instance_' + split + '.json')\n",
        "        annotations = load_json(annotation_path)\n",
        "\n",
        "        for annotation in annotations['annotations']:\n",
        "            if annotation['image_name'] not in self.f_names:\n",
        "                if len(self.f_names) != 0:\n",
        "                    self.boxes.append(torch.Tensor(box))\n",
        "                    self.labels.append(torch.LongTensor(label))\n",
        "                box, label = [], []\n",
        "                self.f_names.append(annotation['image_name'])\n",
        "\n",
        "            bbox = annotation['bbox']\n",
        "            x1, y1, x2, y2 = float(bbox[0]), float(bbox[1]), float(bbox[0] + bbox[2]), float(bbox[1] + bbox[3])\n",
        "            box.append([x1, y1, x2, y2])\n",
        "            label.append(int(annotation['category_id']))\n",
        "\n",
        "        self.num_samples = len(self.boxes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        f_name = self.f_names[idx]\n",
        "        img = cv2.imread(os.path.join(self.root_images, f_name))\n",
        "        boxes = self.boxes[idx].clone()\n",
        "        labels = self.labels[idx].clone()\n",
        "\n",
        "        if self.train:\n",
        "            # img = self.random_bright(img)\n",
        "            img, boxes = random_flip(img, boxes)\n",
        "            img, boxes = randomScale(img, boxes)\n",
        "            img = randomBlur(img)\n",
        "            img = RandomBrightness(img)\n",
        "            img = RandomHue(img)\n",
        "            img = RandomSaturation(img)\n",
        "            img, boxes, labels = randomShift(img, boxes, labels)\n",
        "            img, boxes, labels = randomCrop(img, boxes, labels)\n",
        "\n",
        "        h, w, _ = img.shape\n",
        "        boxes /= torch.Tensor([w, h, w, h]).expand_as(boxes)\n",
        "        img = BGR2RGB(img)\n",
        "        img = subMeanDividedStd(img, self.mean, self.std)\n",
        "        img = cv2.resize(img, (self.image_size, self.image_size))\n",
        "        target = self.encoder(boxes, labels)  # S*S*(B*5+C)\n",
        "        for t in self.transform:\n",
        "            img = t(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def encoder(self, boxes, labels):\n",
        "        S, B, C = 14, 2, 5\n",
        "        grid_num = S\n",
        "        target = torch.zeros((grid_num, grid_num, B * 5 + C))\n",
        "        cell_size = 1.0 / grid_num\n",
        "        wh = boxes[:, 2:] - boxes[:, :2]\n",
        "        cxcy = (boxes[:, 2:] + boxes[:, :2]) / 2\n",
        "        for i in range(cxcy.size()[0]):\n",
        "            cxcy_sample = cxcy[i]\n",
        "            ij = (cxcy_sample / cell_size).ceil() - 1\n",
        "            target[int(ij[1]), int(ij[0]), 4] = 1\n",
        "            target[int(ij[1]), int(ij[0]), 9] = 1\n",
        "            target[int(ij[1]), int(ij[0]), int(labels[i]) + 9] = 1\n",
        "            xy = ij * cell_size\n",
        "            delta_xy = (cxcy_sample - xy) / cell_size\n",
        "            target[int(ij[1]), int(ij[0]), 2:4] = wh[i]\n",
        "            target[int(ij[1]), int(ij[0]), :2] = delta_xy\n",
        "            target[int(ij[1]), int(ij[0]), 7:9] = wh[i]\n",
        "            target[int(ij[1]), int(ij[0]), 5:7] = delta_xy\n",
        "        return target\n"
      ],
      "metadata": {
        "id": "_V9fLP0ofjqg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOLO Loss"
      ],
      "metadata": {
        "id": "B5KdVkjf2TlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# criterion = yololoss(args, l_coord=args.l_coord, l_noobj=args.l_noobj)\n",
        "class yololoss(nn.Module):\n",
        "    def __init__(self, args, l_coord, l_noobj):\n",
        "        super(yololoss, self).__init__()\n",
        "        self.S = args['yolo_S']\n",
        "        self.B = args['yolo_B']\n",
        "        self.C = args['yolo_C']\n",
        "        self.len_pred = (5 * self.B) + self.C\n",
        "        self.l_coord = l_coord\n",
        "        self.l_noobj = l_noobj\n",
        "\n",
        "    def compute_iou(self, box1, box2):\n",
        "        \"\"\" compute IOU between boxes\n",
        "            - box1 (bs, 4)  4: [x1, y1, x2, y2]  left top and right bottom\n",
        "            - box2 (bs, 4)  4: [x1, y1, x2, y2]  left top and right bottom\n",
        "        \"\"\"\n",
        "        N = box1.size(0)\n",
        "        M = box2.size(0)\n",
        "        \n",
        "        lt = torch.max(\n",
        "            box1[:, :2].unsqueeze(1).expand(N, M, 2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
        "            box2[:, :2].unsqueeze(0).expand(N, M, 2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
        "        )\n",
        "\n",
        "        rb = torch.min(\n",
        "            box1[:, 2:].unsqueeze(1).expand(N, M, 2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
        "            box2[:, 2:].unsqueeze(0).expand(N, M, 2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
        "        )\n",
        "\n",
        "        wh = rb - lt  # [N,M,2]\n",
        "        wh[wh < 0] = 0  # clip at 0\n",
        "        inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "\n",
        "        area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])  # [N,]\n",
        "        area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])  # [M,]\n",
        "        area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N,1] -> [N,M]\n",
        "        area2 = area2.unsqueeze(0).expand_as(inter)  # [M,] -> [1,M] -> [N,M]\n",
        "\n",
        "        iou = inter / (area1 + area2 - inter)\n",
        "        return iou\n",
        "\n",
        "    def forward(self, prediction, target):\n",
        "        \"\"\"   [1, 14, 14, 15]\n",
        "            - prediction: (bs, S, S, B*5+C)    [x1, y1, w1, h1, c1, x2, y2, w2, h2, c2, confidence for C classes]\n",
        "            - target: (bs, S, S, B*5+C)    [x, y, w, h, c, x, y, w, h, c, confidence for C classes]\n",
        "        \"\"\"        \n",
        "\n",
        "        '''\n",
        "        coo_mask torch.Size([12, 14, 14])\n",
        "        noo_mask torch.Size([12, 14, 14])\n",
        "        coo_mask torch.Size([12, 14, 14, 15])\n",
        "        coo_pred torch.Size([74, 15])\n",
        "        box_pred torch.Size([148, 5])\n",
        "        class_pred torch.Size([74, 5])\n",
        "        '''\n",
        "        N = prediction.size()[0]\n",
        "        coo_mask = target[:, :, :, 4] > 0     # 有目标[1, 14, 14] [bool type]\n",
        "        noo_mask = target[:, :, :, 4] == 0    # 无目标[1, 14, 14] \n",
        "        \n",
        "        coo_mask = coo_mask.unsqueeze(-1).expand_as(target)   # [1, 14, 14, 15]\n",
        "        noo_mask = noo_mask.unsqueeze(-1).expand_as(target)   # [1, 14, 14, 15]\n",
        "\n",
        "        coo_pred = prediction[coo_mask].view(-1, self.len_pred)             # [bs中所有的the prediction # grid that contain the object的和 , 15]\n",
        "        box_pred = coo_pred[:, :self.B * 5].contiguous().view(-1, 5)        # [bs*the prediction # grid that contain the object*2, 5] grids [[x1,y1,w1,h1,c1],[x2,y2,w2,h2,c2], [],[], [],[]]\n",
        "        class_pred = coo_pred[:, self.B * 5:]                               # [bs*the prediction # grid that contain the object, 5]\n",
        "        \n",
        "        coo_target = target[coo_mask].view(-1, self.len_pred)               # [the truth # grid that contain the object, 15]\n",
        "        box_target = coo_target[:, :self.B * 5].contiguous().view(-1, 5)    # [the truth # grid that contain the object*2, 5] boxes [[x1,y1,w1,h1,c1],[x2,y2,w2,h2,c2], [],[], [],[]]\n",
        "        class_target = coo_target[:, self.B * 5:]                           # [the truth # grid that contain the object, 5]\n",
        "        \n",
        "        \"\"\"Non Maximum Suppression\"\"\"\n",
        "        coo_response_mask = torch.cuda.BoolTensor(box_target.size())        # [[F,F,F,F,F],[F,F,F,F,F], [],[], [],[]]\n",
        "        coo_response_mask.zero_()                                             # 全变成False\n",
        "\n",
        "        coo_not_response_mask = torch.cuda.BoolTensor(box_target.size())    # coo_response_mask 和 coo_not_response_mask 是含有目标的box\n",
        "        coo_not_response_mask.zero_()\n",
        "        box_target_iou = torch.zeros(box_target.size()).cuda()              # [[0,0,0,0,0],[0,0,0,0,0], [],[], [],[]]\n",
        "\n",
        "        for i in range(0, box_target.size()[0], self.B):   # (0, # truth that contain*2, 2)\n",
        "            box1 = box_pred[i:i + self.B]                  # [[x1,y1,w1,h1,c1],[x2,y2,w2,h2,c2]]\n",
        "            box1_xyxy = torch.FloatTensor(box1.size())     # [2, 5]\n",
        "            \"\"\" from [x,y,w,h] to [x1,y1,x2,y2]\"\"\"\n",
        "            box1_xyxy[:, :2] = box1[:, :2] / self.S - 0.5 * box1[:, 2:4]           # [[x1_left, y1_left, x1_right, y1_right, c1]\n",
        "                                                                                   #  [x2_left, y2_left, x2_right, y2_right, c2]]\n",
        "            box1_xyxy[:, 2:4] = box1[:, :2] / self.S + 0.5 * box1[:, 2:4]\n",
        "            \n",
        "            box1 = box1_xyxy[:,:4]\n",
        "            \n",
        "            # box2 = box_target[i].view(-1, 5)             # ？？？？？？？？？？这里我不理解？？？？？？？？？？？？？？？？？\n",
        "            # box2_xyxy = torch.FloatTensor(box2.size())   # ？？？？？？？？？这两行原码未注释？？？？？？？？？？？？？？？？\n",
        "            ###################################################################\n",
        "            # TODO: Please fill the codes below to calculate the iou of the two boxes and substite the \"?\"\n",
        "            # Note: return variable: iou_res (self.B, 1)\n",
        "            ##################################################################\n",
        "            box2 = box_target[i:i + self.B]                  # [[x1,y1,w1,h1,c1],[x2,y2,w2,h2,c2]]\n",
        "            box2_xyxy = torch.FloatTensor(box2.size())       # [2, 5]\n",
        "            \"\"\" from [x,y,w,h] to [x1,y1,x2,y2]\"\"\"\n",
        "            box2_xyxy[:, :2] = box2[:, :2] / self.S - 0.5 * box2[:, 2:4]  # [[x1_left, y1_left, x1_right, y1_right, c1]\n",
        "                                                                          #  [x2_left, y2_left, x2_right, y2_right, c2]]\n",
        "\n",
        "            box2_xyxy[:, 2:4] = box2[:, :2] / self.S + 0.5 * box2[:, 2:4]\n",
        "            box2 = box2_xyxy[:, 0:4]\n",
        "            iou_res = self.compute_iou(box2, box1)[0]\n",
        "            ##################################################################\n",
        "            max_iou, max_index = iou_res.max(0)\n",
        "            max_index = max_index.data.cuda()\n",
        "\n",
        "            coo_response_mask[i + max_index] = True\n",
        "            \n",
        "            for j in range(self.B):\n",
        "                if j == max_index:\n",
        "                    continue\n",
        "                else:\n",
        "                    coo_not_response_mask[i + j] = True\n",
        "\n",
        "            box_target_iou[i + max_index, torch.LongTensor([4]).cuda()] = max_iou.data.cuda()\n",
        "        \n",
        "        box_target_iou = box_target_iou.cuda()\n",
        "        \n",
        "        \"\"\"Compute Term1 + Term2: Location Loss\"\"\"\n",
        "        box_pred = box_pred.cuda()                                              # [[], [], [], ..., []]\n",
        "        box_target = box_target.cuda()\n",
        "        box_pred_response = box_pred[coo_response_mask].view(-1, 5)\n",
        "        box_target_response = box_target[coo_response_mask].view(-1, 5)\n",
        "        ###################################################################\n",
        "        # TODO: Please fill the codes below to calculate the location loss\n",
        "        ##################################################################\n",
        "        loc_loss = 0\n",
        "        x_2 = torch.square(box_pred[:,0] - box_target[:, 0])\n",
        "        y_2 = torch.square(box_pred[:,1] - box_target[:, 1])\n",
        "        loc_loss += self.l_coord*(torch.sum(x_2 + y_2))\n",
        "\n",
        "        w = torch.square(torch.sqrt(box_pred[:,2]) - torch.sqrt(box_target[:, 2]))\n",
        "        h = torch.square(torch.sqrt(box_pred[:,3]) - torch.sqrt(box_target[:, 3]))\n",
        "        loc_loss += self.l_coord*(torch.sum(w+h))\n",
        "        ##################################################################\n",
        "        \n",
        "        \"\"\"Compute the 3rd Term: IOU loss for boxes containing the objects\"\"\"\n",
        "        box_target_response_iou = box_target_iou[coo_response_mask].view(-1, 5)\n",
        "        contain_loss = F.mse_loss(box_pred_response[:, 4], box_target_response_iou[:, 4], reduction='sum')\n",
        "        \n",
        "        \"\"\"Compute the 4th Term (Part I): Not Response Loss\"\"\"\n",
        "        ###################################################################\n",
        "        # TODO: Please fill the codes below to calculate the Not Response Loss   \n",
        "        ##################################################################\n",
        "        box_pred_not_response = box_pred[coo_not_response_mask].view(-1, 5)\n",
        "        box_target_not_response = box_target[coo_not_response_mask].view(-1, 5)\n",
        "        not_response_loss = F.mse_loss(box_pred_not_response[:, 4], box_target_not_response[:, 4], reduction = 'sum')\n",
        "        ##################################################################\n",
        "        \n",
        "        \"\"\"Compute the 4th Term (Part II): No Object Contain Loss\"\"\"\n",
        "        noo_pred = prediction[noo_mask].view(-1, self.len_pred).cuda()\n",
        "        noo_target = target[noo_mask].view(-1, self.len_pred).cuda()\n",
        "        noo_pred_mask = torch.cuda.BoolTensor(noo_pred.size()).cuda()\n",
        "        noo_pred_mask.zero_()\n",
        "        noo_pred_mask[:, 4] = 1\n",
        "        noo_pred_mask[:, 9] = 1\n",
        "        noo_pred_c = noo_pred[noo_pred_mask]\n",
        "        noo_target_c = noo_target[noo_pred_mask]\n",
        "        nooobj_loss = F.mse_loss(noo_pred_c, noo_target_c, reduction='sum')\n",
        "        \n",
        "        \"\"\"Compute the 5th Term: Class Loss\"\"\"\n",
        "        class_loss = F.mse_loss(class_pred, class_target, reduction='sum')\n",
        "        \n",
        "        \"\"\"Summarize the five terms\"\"\"\n",
        "        loss = self.l_coord * loc_loss + 2 * contain_loss + not_response_loss + self.l_noobj * nooobj_loss + class_loss\n",
        "\n",
        "        return loss / N\n",
        "loss = yololoss(args, 5, 0.5)"
      ],
      "metadata": {
        "id": "M-GMUTT22XY9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train"
      ],
      "metadata": {
        "id": "Nedl1D1Xg_wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_root = '/content/deeplearning_assignment_1_dataset'\n",
        "train_dataset = Dataset(file_root, split='train',\n",
        "                        transform=[transforms.ToTensor()])\n",
        "train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, num_workers=os.cpu_count() - 2)\n",
        "\n",
        "val_dataset = Dataset(args['dataset_root'], split='val', transform=[transforms.ToTensor()])\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args['batch_size'], shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI4UtG5MeE47",
        "outputId": "0f218e06-4340-4321-e2b4-843eeb883c34"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATASET INITIALIZATION\n",
            "DATASET INITIALIZATION\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLpOYFqFCoKT",
        "outputId": "f84c8424-2eb7-4892-9426-c124c48b2e67"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pretrained(net):\n",
        "    resnet = torchvision.models.resnet50(pretrained=True)\n",
        "    resnet_state_dict = resnet.state_dict()\n",
        "\n",
        "    net_dict = net.state_dict()\n",
        "    for k in resnet_state_dict.keys():\n",
        "        if k in net_dict.keys() and not k.startswith('fc'):\n",
        "            net_dict[k] = resnet_state_dict[k]\n",
        "    net.load_state_dict(net_dict)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('NUMBER OF CUDA DEVICES:', torch.cuda.device_count())\n",
        "\n",
        "# Other settings\n",
        "args['load_pretrain'] = False\n",
        "print(args)\n",
        "\n",
        "output_dir = args['output_dir']\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "np.random.seed(args['seed'])\n",
        "torch.manual_seed(args['seed'])\n",
        "\n",
        "####################################################################\n",
        "criterion = yololoss(args, l_coord=args['l_coord'], l_noobj=args['l_noobj'])\n",
        "\n",
        "hku_mmdetector = resnet50(args=args)\n",
        "if args['load_pretrain']:\n",
        "    load_pretrained(hku_mmdetector)\n",
        "hku_mmdetector = hku_mmdetector.to(device)\n",
        "\n",
        "hku_mmdetector.train()\n",
        "\n",
        "# initialize optimizer\n",
        "optimizer = torch.optim.AdamW(hku_mmdetector.parameters(), betas=(0.9, 0.999), lr=args['learning_rate'])\n",
        "\n",
        "# initialize dataset\n",
        "train_dataset = Dataset(args['dataset_root'], split='train', transform=[transforms.ToTensor()])\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, num_workers=4)\n",
        "\n",
        "###################################################################\n",
        "# TODO: Please fill the codes below to initialize the validation dataset\n",
        "##################################################################\n",
        "val_dataset = Dataset(args['dataset_root'], split='val', transform=[transforms.ToTensor()])\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args['batch_size'], shuffle=False, num_workers=4)\n",
        "##################################################################\n",
        "bs = args['batch_size']\n",
        "print(f'NUMBER OF DATA SAMPLES: {len(train_dataset)}')\n",
        "print(f'BATCH SIZE: {bs}')\n",
        "\n",
        "train_dict = dict(iter=[], loss=[])\n",
        "best_val_loss = np.inf\n",
        "lossList = []\n",
        "for epoch in range(args['num_epochs']):\n",
        "    hku_mmdetector.train()\n",
        "\n",
        "    # training\n",
        "    total_loss = 0.\n",
        "    print(('\\n' + '%10s' * 3) % ('epoch', 'loss', 'gpu'))\n",
        "    progress_bar = tqdm.tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for i, (images, target) in progress_bar:\n",
        "        images = images.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        pred = hku_mmdetector(images)\n",
        "        loss = criterion(pred, target)\n",
        "\n",
        "        total_loss += loss.data\n",
        "        lossList.append(loss.data)\n",
        "        ###################################################################\n",
        "        # TODO: Please fill the codes here to complete the gradient backward\n",
        "        ##################################################################\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        ##################################################################\n",
        "\n",
        "        mem = '%.3gG' % (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)\n",
        "        s = ('%10s' + '%10.4g' + '%10s') % ('%g/%g' % (epoch + 1, args['num_epochs']), total_loss / (i + 1), mem)\n",
        "        progress_bar.set_description(s)\n",
        "\n",
        "    # validation\n",
        "    validation_loss = 0.0\n",
        "    hku_mmdetector.eval()\n",
        "    progress_bar = tqdm.tqdm(enumerate(val_loader), total=len(val_loader))\n",
        "    for i, (images, target) in progress_bar:\n",
        "        images = images.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        prediction = hku_mmdetector(images)\n",
        "        loss = criterion(prediction, target)\n",
        "        validation_loss += loss.data\n",
        "    validation_loss /= len(val_loader)\n",
        "    print(\"validation loss:\", validation_loss.item())\n",
        "\n",
        "    if best_val_loss > validation_loss:\n",
        "        best_val_loss = validation_loss\n",
        "\n",
        "        save = {'state_dict': hku_mmdetector.state_dict()}\n",
        "        torch.save(save, os.path.join(output_dir, 'hku_mmdetector_best.pth'))\n",
        "\n",
        "    save = {'state_dict': hku_mmdetector.state_dict()}\n",
        "    torch.save(save, os.path.join(output_dir, 'hku_mmdetector_epoch_'+str(epoch+1)+'.pth'))\n",
        "\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "gcOlT2-8ppa5",
        "outputId": "9468fb36-c942-4111-ba34-e201f4f44f1e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NUMBER OF CUDA DEVICES: 1\n",
            "{'yolo_S': 14, 'yolo_B': 2, 'yolo_C': 5, 'num_epochs': 1, 'batch_size': 20, 'learning_rate': 1e-05, 'seed': 666, 'dataset_root': '/content/deeplearning_assignment_1_dataset', 'output_dir': '/content/checkpoints', 'l_coord': 5.0, 'l_noobj': 0.5, 'nms_threshold': 0.5, 'image_path': '/content/deeplearning_assignment_1_dataset/val/image/000001.jpg', 'model_path': '/content/checkpoints/hku_mmdetector_best.pth', 'unsave_img': False, 'vis_dir': '/content/vis_results', 'split': 'val', 'output_file': '/content/checkpoints/result.pkl', 'pos_threshold': 0.3, 'load_pretrain': False}\n",
            "DATASET INITIALIZATION\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATASET INITIALIZATION\n",
            "NUMBER OF DATA SAMPLES: 4964\n",
            "BATCH SIZE: 20\n",
            "\n",
            "     epoch      loss       gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/1     146.4     8.81G: 100%|██████████| 249/249 [09:52<00:00,  2.38s/it]\n",
            "  2%|▏         | 3/125 [00:07<05:07,  2.52s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b8ae0702ad77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhku_mmdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-5968a4aec29a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 14.76 GiB total capacity; 13.35 GiB already allocated; 37.88 MiB free; 13.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non_maximum_suppression"
      ],
      "metadata": {
        "id": "y8jkKUfxgUr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# boxes.shape = [367, 4], cls_indexes.shape = [367], confidences.shape = [367]\n",
        "def non_maximum_suppression(boxes, scores, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        - boxes: (bs, 4)  4: [x1, y1, x2, y2] left top and right bottom\n",
        "        - scores: (bs, )   confidence score\n",
        "        - threshold: int    delete bounding box with IoU greater than threshold\n",
        "    Return:\n",
        "        - A long int tensor whose size is (bs, )\n",
        "    \"\"\"\n",
        "    ###################################################################\n",
        "    # TODO: Please fill the codes below to calculate the iou of the two boxes\n",
        "    # Hint: You can refer to the nms part implemented in loss.py but the input shapes are different here\n",
        "    ##################################################################\n",
        "    if len(boxes) == 0:\n",
        "        return [], []\n",
        "\n",
        "    # coordinates of bounding boxes\n",
        "    start_x = boxes[:, 0]\n",
        "    start_y = boxes[:, 1]\n",
        "    end_x = boxes[:, 2]\n",
        "    end_y = boxes[:, 3]\n",
        "\n",
        "    # Confidence scores of bounding boxes\n",
        "\n",
        "    # Picked bounding boxes\n",
        "    picked_index = []\n",
        "\n",
        "    # Compute areas of bounding boxes\n",
        "    areas = (end_x - start_x + 1) * (end_y - start_y + 1)\n",
        "\n",
        "    # Sort by confidence score of bounding boxes\n",
        "    order = np.argsort(scores)\n",
        "\n",
        "    # Iterate bounding boxes\n",
        "    while order.shape[0] > 0:\n",
        "        # The index of largest confidence score\n",
        "        index = order[-1]\n",
        "\n",
        "        # Pick the bounding box with largest confidence score\n",
        "        picked_index.append(index)\n",
        "\n",
        "        # Compute ordinates of intersection-over-union(IOU)\n",
        "        x1 = np.maximum(start_x[index], start_x[order[:-1]])\n",
        "        x2 = np.minimum(end_x[index], end_x[order[:-1]])\n",
        "        y1 = np.maximum(start_y[index], start_y[order[:-1]])\n",
        "        y2 = np.minimum(end_y[index], end_y[order[:-1]])\n",
        "\n",
        "        # Compute areas of intersection-over-union\n",
        "        w = np.maximum(0.0, x2 - x1 + 1)\n",
        "        h = np.maximum(0.0, y2 - y1 + 1)\n",
        "        intersection = w * h\n",
        "\n",
        "        # Compute the ratio between intersection and union\n",
        "        ratio = intersection / (areas[index] + areas[order[:-1]] - intersection)\n",
        "\n",
        "        left = np.where(ratio < threshold)\n",
        "        order = order[left]\n",
        "\n",
        "    return torch.tensor(picked_index)\n",
        "    ##################################################################\n",
        "\n",
        "\n",
        "def pred2box(args, prediction):     # 1xSxSx(B*5+C)\n",
        "    \"\"\"\n",
        "    This function calls non_maximum_suppression to transfer predictions to predicted boxes.\n",
        "    \"\"\"\n",
        "    S, B, C = args['yolo_S'], args['yolo_B'], args['yolo_C']\n",
        "    \n",
        "    boxes, cls_indexes, confidences = [], [], []\n",
        "    prediction = prediction.data.squeeze(0)  # [14, 14, 15]\n",
        "    \n",
        "    contain = [] \n",
        "    for b in range(B):\n",
        "        tmp_contain = prediction[:, :, b * 5 + 4].unsqueeze(2) # confidences: [14, 14, 1]\n",
        "        contain.append(tmp_contain)\n",
        "\n",
        "    contain = torch.cat(contain, 2) # [14, 14, 2] 每个位置上有两个 confidence\n",
        "    mask1 = contain > 0.1\n",
        "    mask2 = (contain == contain.max())\n",
        "    mask = mask1 + mask2           # 同时为真是2，只有一个为真是1，同时为假为0\n",
        "    \n",
        "    for i in range(S):\n",
        "        for j in range(S):\n",
        "            for b in range(B):\n",
        "                if mask[i, j, b] == 1:\n",
        "                    box = prediction[i, j, b * 5:b * 5 + 4]             # 若只有一个为真，那就把对应prediction里的box取出来 -> [x1, y1, w1, h1]\n",
        "                    contain_prob = torch.FloatTensor([prediction[i, j, b * 5 + 4]]) # 把 confidence 取出来\n",
        "                    xy = torch.FloatTensor([j, i]) * 1.0 / S\n",
        "                    box[:2] = box[:2] * 1.0 / S + xy\n",
        "                    box_xy = torch.FloatTensor(box.size())\n",
        "                    box_xy[:2] = box[:2] - 0.5 * box[2:]\n",
        "                    box_xy[2:] = box[:2] + 0.5 * box[2:]\n",
        "                    max_prob, cls_index = torch.max(prediction[i, j, B*5:], 0)\n",
        "                    cls_index = torch.LongTensor([cls_index])\n",
        "                    if float((contain_prob * max_prob)[0]) > 0.1:\n",
        "                        boxes.append(box_xy.view(1, 4))\n",
        "                        cls_indexes.append(cls_index)\n",
        "                        confidences.append(contain_prob * max_prob)\n",
        "\n",
        "    if len(boxes) == 0:\n",
        "        boxes = torch.zeros((1, 4))\n",
        "        confidences = torch.zeros(1)\n",
        "        cls_indexes = torch.zeros(1)\n",
        "    else:\n",
        "        boxes = torch.cat(boxes, 0)\n",
        "        confidences = torch.cat(confidences, 0)\n",
        "        cls_indexes = torch.cat(cls_indexes, 0)\n",
        "    # print('boxes.shape, cls_indexes.shape, confidences.shape',boxes.shape, cls_indexes.shape, confidences.shape)\n",
        "    #     !!!!!!!!!!!!!!!!!!!!!!!!!      # \n",
        "    keep = non_maximum_suppression(boxes, confidences, threshold=args['nms_threshold'])\n",
        "    # print(keep.shape)\n",
        "    return boxes[keep], cls_indexes[keep], confidences[keep]\n",
        "\n",
        "\n",
        "def inference(args, model, img_path):\n",
        "    \"\"\"\n",
        "    Inference the image with trained model to get the predicted bounding boxes\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    img = cv2.imread(img_path)\n",
        "    h, w, _ = img.shape\n",
        "    img = cv2.resize(img, (448, 448))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    mean = (123.675, 116.280, 103.530)  # RGB\n",
        "    std = (58.395, 57.120, 57.375)\n",
        "    ###################################################################\n",
        "    # TODO: Please fill the codes here to do the image normalization\n",
        "    ##################################################################\n",
        "    # Inew = (I - I.mean) / I.std\n",
        "    img[0, :, :] = (img[0, :, :] - mean[0]) / std[0]\n",
        "    img[1, :, :] = (img[1, :, :] - mean[1]) / std[1]\n",
        "    img[2, :, :] = (img[2, :, :] - mean[2]) / std[2]\n",
        "    ##################################################################\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor(), ])\n",
        "    img = transform(img).unsqueeze(0)\n",
        "    img = img.cuda()\n",
        "    model.cuda()\n",
        "\n",
        "    # [1, 3, 448, 448]\n",
        "    with torch.no_grad():\n",
        "        prediction = model(img).cpu()  # 1xSxSx(B*5+C)\n",
        "        boxes, cls_indices, confidences = pred2box(args, prediction)\n",
        "    # !!!!!!!!!!!!!!!!!\n",
        "    # print('boxed.shape',boxes.shape)\n",
        "    # print('cls_indices.shape',cls_indices.shape)\n",
        "    # print('confidences.shape',confidences.shape)\n",
        "    for i, box in enumerate(boxes):\n",
        "        x1 = int(box[0] * w)\n",
        "        x2 = int(box[2] * w)\n",
        "        y1 = int(box[1] * h)\n",
        "        y2 = int(box[3] * h)\n",
        "        cls_index = cls_indices[i]\n",
        "        cls_index = int(cls_index)  # convert LongTensor to int\n",
        "        conf = confidences[i]\n",
        "        conf = float(conf)\n",
        "        results.append([(x1, y1), (x2, y2), CAR_CLASSES[cls_index], img_path.split('/')[-1], conf])\n",
        "    # print(results)\n",
        "    return results"
      ],
      "metadata": {
        "id": "dfBd5DW28aAp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = inference(args, hku_mmdetector50, '/content/deeplearning_assignment_1_dataset/val/image/000002.jpg')"
      ],
      "metadata": {
        "id": "hQLTODOtkwSm"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A3Am98x5iFd",
        "outputId": "bd87df17-4ffd-42bb-a62a-a5f6b975f118"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(289, 221), (819, 1028), 'Car', '000002.jpg', 0.9999924898147583],\n",
              " [(1371, 190), (1371, 425), 'Car', '000002.jpg', 0.9999922513961792],\n",
              " [(775, -209), (872, 530), 'Car', '000002.jpg', 0.9999704360961914],\n",
              " [(1858, -222), (1859, 376), 'Car', '000002.jpg', 0.9998140335083008],\n",
              " [(685, 745), (687, 850), 'Car', '000002.jpg', 0.999272346496582],\n",
              " [(1245, 847), (1245, 887), 'Car', '000002.jpg', 0.999169111251831],\n",
              " [(415, 520), (416, 548), 'Car', '000002.jpg', 0.9968346357345581],\n",
              " [(144, -461), (162, 614), 'Car', '000002.jpg', 0.9948241710662842],\n",
              " [(1650, 526), (1656, 774), 'Car', '000002.jpg', 0.9931192398071289],\n",
              " [(-633, 868), (999, 1141), 'Car', '000002.jpg', 0.8075276017189026],\n",
              " [(1753, 907), (1943, 1137), 'Car', '000002.jpg', 0.6508580446243286],\n",
              " [(1088, -7), (2751, 469), 'Car', '000002.jpg', 0.6457266807556152],\n",
              " [(1222, 601), (2341, 1404), 'Car', '000002.jpg', 0.4876706898212433]]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Valid"
      ],
      "metadata": {
        "id": "j9OWswQdcoXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hku_mmdetector = resnet50(args=args)"
      ],
      "metadata": {
        "id": "31a3FHrvhEeC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluation:\n",
        "    def __init__(self, predictions, targets, threshold):\n",
        "        super(Evaluation, self).__init__()\n",
        "        self.predictions = predictions\n",
        "        self.targets = targets\n",
        "        self.threshold = threshold\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_ap(recall, precision):\n",
        "        # average precision calculation\n",
        "        recall = np.concatenate(([0.], [recall], [1.]))\n",
        "        precision = np.concatenate(([0.], [precision], [0.]))\n",
        "\n",
        "        for i in range(precision.size - 1, 0, -1):\n",
        "            precision[i - 1] = max(precision[i - 1], precision[i])\n",
        "\n",
        "        ap = 0.0  # average precision (AUC of the precision-recall curve).\n",
        "        for i in range(precision.size - 1):\n",
        "            ap += (recall[i + 1] - recall[i]) * precision[i + 1]\n",
        "\n",
        "        return ap\n",
        "\n",
        "    # targets = {('00001.jpg', 'Cyclist'):[[x1, y1, x2, y2], [], []], (,):[[], []]}\n",
        "    # predictions:{'Cyclist':[['00001.jpg', conf, x1, y1, x2, y2], [], []], ''}\n",
        "    # aps = Evaluation(predictions, targets, threshold=args['pos_threshold']).evaluate()\n",
        "    def evaluate(self):\n",
        "        aps = []\n",
        "        print('CLASS'.ljust(25, ' '), 'AP')\n",
        "        for class_name in CAR_CLASSES:\n",
        "            class_preds = self.predictions[class_name]  # [[image_name,confidence,x1,y1,x2,y2],...]\n",
        "            if len(class_preds) == 0:\n",
        "                ap = 0\n",
        "                print(f'{class_name}'.ljust(25, ' '), f'{ap:.2f}')\n",
        "                aps.append(ap)\n",
        "                continue\n",
        "            image_ids = [x[0] for x in class_preds]                        # ['00001.jpg', '']\n",
        "            confidence = np.array([float(x[1]) for x in class_preds])      # [0.99, 0.98]\n",
        "            BB = np.array([x[2:] for x in class_preds])                    # [[x1, y1, x2, y2], [], []]\n",
        "            # sort by confidence\n",
        "            sorted_ind = np.argsort(-confidence)                           # 降序index\n",
        "            sorted_scores = np.sort(-confidence)                           \n",
        "            BB = BB[sorted_ind, :]                                         # 降序BB\n",
        "            image_ids = [image_ids[x] for x in sorted_ind]                 # 降序id\n",
        "\n",
        "            # go down dets and mark TPs and FPs\n",
        "            npos = 0.          # target里属于当前类的所有的BB的个数\n",
        "            truth_images_num = 0\n",
        "            for (key1, key2) in self.targets:\n",
        "                if key2 == class_name:\n",
        "                    npos += len(self.targets[(key1, key2)])\n",
        "                    truth_images_num += 1\n",
        "            nd = len(image_ids) # 图片个数\n",
        "            tp = np.zeros(nd)\n",
        "            fp = np.zeros(nd)\n",
        "\n",
        "            for d, image_id in enumerate(image_ids):       # 当前预测predict里认为是当前类别的所有image\n",
        "                bb = BB[d]                                 # predict的bb\n",
        "                if (image_id, class_name) in self.targets: # 如果target里有这个组合，说明预测的也许是对的\n",
        "                    print('have probability')\n",
        "                    BBGT = self.targets[(image_id, class_name)]\n",
        "                    for x1y1_x2y2 in BBGT:\n",
        "                        # compute overlaps\n",
        "                        # intersection\n",
        "                        x_min = np.maximum(x1y1_x2y2[0], bb[0])\n",
        "                        y_min = np.maximum(x1y1_x2y2[1], bb[1])\n",
        "                        x_max = np.minimum(x1y1_x2y2[2], bb[2])\n",
        "                        y_max = np.minimum(x1y1_x2y2[3], bb[3])\n",
        "                        w = np.maximum(x_max - x_min + 1., 0.)\n",
        "                        h = np.maximum(y_max - y_min + 1., 0.)\n",
        "                        intersection = w * h\n",
        "\n",
        "                        union = (bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) + (x1y1_x2y2[2] - x1y1_x2y2[0] + 1.) * (\n",
        "                                x1y1_x2y2[3] - x1y1_x2y2[1] + 1.) - intersection\n",
        "                        if union == 0:\n",
        "                            print(bb, x1y1_x2y2)\n",
        "\n",
        "                        overlaps = intersection / union\n",
        "                        if overlaps > self.threshold:\n",
        "                            tp[d] = 1\n",
        "                            BBGT.remove(x1y1_x2y2)\n",
        "                            if len(BBGT) == 0:\n",
        "                                del self.targets[(image_id, class_name)]\n",
        "                            break\n",
        "                    fp[d] = 1 - tp[d]\n",
        "                else:     # target没有这个组合，一定是错的，所以fp+1\n",
        "                    fp[d] = 1\n",
        "            ###################################################################\n",
        "            # TODO: Please fill the codes to compute recall and precision\n",
        "            ##################################################################\n",
        "            print('tp',tp)\n",
        "            print('fp', fp)\n",
        "            truth_images_num\n",
        "            recall = 0.\n",
        "            precision = 0.\n",
        "            recall = (np.sum(tp) / truth_images_num)\n",
        "            precision = (np.sum(tp) / nd)\n",
        "            print('recall', recall)\n",
        "            print('precision', precision)\n",
        "            ##################################################################\n",
        "            ap = self.compute_ap(recall, precision)\n",
        "            print(f'{class_name}'.ljust(25, ' '), f'{ap*100:.2f}')\n",
        "            aps.append(ap)\n",
        "        print('aps', aps)\n",
        "        return aps\n",
        "\n"
      ],
      "metadata": {
        "id": "dTNxZs0_ctcs"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = defaultdict(list)\n",
        "predictions = defaultdict(list)\n",
        "image_list = []\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('DATA PREPARING...')\n",
        "annotation_path = os.path.join(args['dataset_root'], 'annotations', 'instance_%s.json' % args['split'])\n",
        "annotations = load_json(annotation_path)\n",
        "\n",
        "for annotation in annotations['annotations']:\n",
        "    image_name = annotation['image_name']\n",
        "    if image_name not in image_list:\n",
        "        image_list.append(image_name)\n",
        "    bbox = annotation['bbox']\n",
        "    x1, y1, x2, y2 = int(bbox[0]), int(bbox[1]), int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])\n",
        "    c = int(annotation['category_id'])\n",
        "    class_name = CAR_CLASSES[c-1]\n",
        "    targets[(image_name, class_name)].append([x1, y1, x2, y2])    # targets = {('00001.jpg', 'Cyclist'):[[x1, y1, x2, y2], [], []], (,):[[], []]}\n",
        "print('DONE.')\n",
        "print('START EVALUATION...')\n",
        "print('len(targets)',len(targets))\n",
        "model = resnet50(args=args).to(device)\n",
        "\n",
        "# if torch.cuda.device_count() > 1:\n",
        "#     model = nn.DataParallel(model)\n",
        "\n",
        "# model.load_state_dict(torch.load(args.model_path)['state_dict'])\n",
        "model.eval()\n",
        "\n",
        "for image_name in image_list: # 原来是tqdm(image_list)\n",
        "    image_path = os.path.join(args['dataset_root'], args['split'], 'image', image_name)\n",
        "    result = inference(args, model, image_path)\n",
        "    # # [(x1, y1), (x2, y2), CAR_CLASSES[cls_index], image_name, conf]\n",
        "    for (x1, y1), (x2, y2), class_name, image_name, conf in result:\n",
        "        predictions[class_name].append([image_name, conf, x1, y1, x2, y2]) # predictions:{'Cyclist':[['00001.jpg', conf, x1, y1, x2, y2], [], []], ''}\n",
        "aps = Evaluation(predictions, targets, threshold=args['pos_threshold']).evaluate()\n",
        "print(f'mAP: {np.mean(aps):.2f}')\n",
        "# write the prediction result\n",
        "f = open(args['output_file'], 'wb')\n",
        "pickle.dump(args, f)\n",
        "pickle.dump(predictions, f)\n",
        "f.close()\n",
        "print(predictions.shape)\n",
        "print('BEGIN CALCULATE MAP...')\n",
        "# aps = Evaluation(predictions, targets, threshold=args['pos_threshold']).evaluate()\n",
        "# print(f'mAP: {np.mean(aps):.2f}')\n",
        "print('DONE.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "CaBBieS7luxO",
        "outputId": "e69c572a-eda8-4229-f211-40a47b22ec66"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA PREPARING...\n",
            "DONE.\n",
            "START EVALUATION...\n",
            "len(targets) 5577\n",
            "CLASS                     AP\n",
            "recall 0.0\n",
            "precision 0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-a85d62dfd764>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# predictions:{'Cyclist':[['00001.jpg', conf, x1, y1, x2, y2], [], []], ''}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos_threshold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'mAP: {np.mean(aps):.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# write the prediction result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-63245ded0430>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m##################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_ap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{class_name}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{ap*100:.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0maps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-63245ded0430>\u001b[0m in \u001b[0;36mcompute_ap\u001b[0;34m(recall, precision)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_ap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# average precision calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 0 dimension(s)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aps = Evaluation(predictions, targets, threshold=args['pos_threshold']).evaluate()"
      ],
      "metadata": {
        "id": "pQdrd2DC8d-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iuNEEZfXicBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "cHM_XVEScwj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(args, model):\n",
        "    image_path = args.image_path\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    print('PREDICTING...')\n",
        "    result = inference(args, model, image_path)\n",
        "\n",
        "    for x1y1, x2y2, class_name, _, prob in result:\n",
        "        color = COLORS[class_name]\n",
        "        cv2.rectangle(image, x1y1, x2y2, color, 2)\n",
        "\n",
        "        label = class_name + str(round(prob, 2))\n",
        "        text_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n",
        "\n",
        "        p1 = (x1y1[0], x1y1[1] - text_size[1])\n",
        "        cv2.rectangle(image, (p1[0] - 2 // 2, p1[1] - 2 - baseline), (p1[0] + text_size[0], p1[1] + text_size[1]),\n",
        "                      color, -1)\n",
        "        cv2.putText(image, label, (p1[0], p1[1] + baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, 8)\n",
        "\n",
        "    if not args.unsave_img:\n",
        "        vis_dir = args.vis_dir\n",
        "        if not os.path.exists(vis_dir):\n",
        "            os.makedirs(vis_dir)\n",
        "        save_path = os.path.join(vis_dir, image_path.split('/')[-1])\n",
        "        cv2.imwrite(save_path, image)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--yolo_S', default=14, type=int, help='YOLO grid num')\n",
        "    parser.add_argument('--yolo_B', default=2, type=int, help='YOLO box num')\n",
        "    parser.add_argument('--yolo_C', default=5, type=int, help='detection class num')\n",
        "\n",
        "    parser.add_argument('--image_path', default=\"./ass1_dataset/val/image/000001.jpg\", help='Path to Image file')\n",
        "    parser.add_argument('--model_path', default=\"./checkpoints/hku_mmdetector_best.pth\", help='Pretrained Model Path')\n",
        "    parser.add_argument('--unsave_img', action='store_true', help='Do not save the image after detection')\n",
        "    parser.add_argument('--vis_dir', default=\"./vis_results\", help='Dir for Visualization')\n",
        "\n",
        "    parser.add_argument('--nms_threshold', default=0.5, type=float, help='Threshold for non maximum suppression')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    ####################################################################\n",
        "    # Prediction\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = resnet50(args=args).to(device)\n",
        "\n",
        "    print('LOADING MODEL...')\n",
        "    #     if torch.cuda.device_count() > 1:\n",
        "    #         model = nn.DataParallel(model)\n",
        "\n",
        "    # If you have single gpu then please modify model loading process\n",
        "    model.load_state_dict(torch.load(args.model_path)['state_dict'])\n",
        "    model.eval()\n",
        "    predict(args, model)"
      ],
      "metadata": {
        "id": "nW1g2JbSc1Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat?"
      ],
      "metadata": {
        "id": "0_ATMUePpFOc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jwm943t5pFsp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}